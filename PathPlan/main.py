import random
from collections import deque
from math import pi

import numpy as np
import torch
from qiskit.quantum_info import random_statevector
from torch import exp, nn, optim, complex128

state_size = 8

# åŠ¨ä½œç©ºé—´å¤§å°ï¼šå‰é¢å®šä¹‰çš„åŠ¨ä½œæ€»æ•°æ˜¯141ç§
action_size = 14

# è¶…å‚æ•°
batch_size = 16  # æ¯æ¬¡è®­ç»ƒç”¨çš„ç»éªŒæ•°é‡
gamma = 0.95  # æŠ˜æ‰£å› å­
epsilon_start = 1.0  # åˆå§‹æ¢ç´¢ç‡
epsilon_end = 0.01  # æœ€å°æ¢ç´¢ç‡
epsilon_decay = 0.995  # æ¢ç´¢ç‡è¡°å‡
learning_rate = 0.001  # å­¦ä¹ ç‡
memory_size = 10000  # ç»éªŒå›æ”¾ç¼“å†²åŒºå¤§å°
episodes = 200  # æ€»å…±è®­ç»ƒå¤šå°‘ä¸ª episode
max_steps = 100  # æ¯ä¸ª episode çš„æœ€å¤§æ­¥æ•°


class QNetwork(nn.Module):
    def __init__(self, state_size, action_size):
        super(QNetwork, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(state_size, 64),
            nn.ReLU(),
            nn.Linear(64, 64),
            nn.ReLU(),
            nn.Linear(64, action_size)
        )

    def forward(self, x):
        return self.net(x)
actions = []
for angle in [np.random.uniform(0, 2 * np.pi), np.random.uniform(0, 2 * np.pi), np.random.uniform(0, 2 * np.pi),
              np.random.uniform(0, 2 * np.pi)]:
    actions.append(('rx', {'angle': angle}))
    actions.append(('ry', {'angle': angle}))
    actions.append(('rz', {'angle': angle}))
# two qubits gate
for from_obj, to_obj in [('Obj1', 'Obj2'), ('Obj2', 'Obj1')]:
    if from_obj or to_obj == 1:
        actions.append(('cnot', {
            'from': from_obj,
            'to': to_obj,
            'cond_bit': 1,
            'flip_bit': 1
        }))

target_state = input_state = random_statevector(2 ** 4)


def flatten_state(state):
    flat = []
    for obj in state:
        flat.extend(obj)
    return flat


def choose_action(model, state, epsilon):
    if np.random.rand() < epsilon:
        return random.randint(0, action_size - 1)  # éšæœºé€‰æ‹©åŠ¨ä½œ
    else:
        with torch.no_grad():
            q_values = model(state)
            return int(torch.argmax(q_values).item())  # é€‰æ‹©æœ€ä¼˜åŠ¨ä½œ


def compute_fidelity_reward(current_state_vector, target_state_vector):
    fidelity = np.abs(np.dot(current_state_vector.conj(), target_state_vector)) ** 2
    return float(fidelity)
def step_1(current_state, action_idx):
    action_type, params = actions[action_idx]
    new_state = apply_operation(current_state, action_type, params)
    # è½¬æ¢ä¸ºå¼ é‡ç§¯æ€
    vector_state = state_to_tensor_product(new_state)
    # è®¡ç®— fidelity
    fidelity = compute_fidelity_reward(vector_state, target_state)
    # reward è®¾ä¸ºè´Ÿçš„ fidelityï¼Œè¿™æ ·æœ€å¤§åŒ– reward å°±ç­‰äºæœ€å°åŒ– fidelity
    reward = -fidelity
    return new_state, reward
class ReplayBuffer:
    def __init__(self, capacity):
        self.buffer = deque(maxlen=capacity)  # ä½¿ç”¨åŒç«¯é˜Ÿåˆ—å­˜å‚¨ç»éªŒ
    def add(self, state, action, reward, next_state, done):
        """æ·»åŠ ä¸€æ¡ç»éªŒ"""
        self.buffer.append((state, action, reward, next_state, done))
    def sample(self, batch_size):
        """éšæœºé‡‡æ ·ä¸€æ‰¹ç»éªŒ"""
        return random.sample(self.buffer, batch_size)
    def size(self):
        """è¿”å›å½“å‰ç¼“å†²åŒºä¸­çš„ç»éªŒæ•°é‡"""
        return len(self.buffer)
def train(model, target_model, optimizer, buffer, batch_size, gamma):
    """
    ä»ç»éªŒå›æ”¾ç¼“å†²åŒºä¸­é‡‡æ ·å¹¶è®­ç»ƒæ¨¡å‹
    :param model: ä¸»ç½‘ç»œ
    :param target_model: ç›®æ ‡ç½‘ç»œ
    :param optimizer: ä¼˜åŒ–å™¨
    :param buffer: ç¼“å†²åŒº
    :param batch_size: æ‰¹é‡å¤§å°
    :param gamma: æŠ˜æ‰£å› å­
    """
    a = buffer.size()
    if a < batch_size:
        return

    # é‡‡æ ·ä¸€æ‰¹ç»éªŒ
    batch = buffer.sample(batch_size)
    states, actions, rewards, next_states, dones = zip(*batch)

    # è½¬æ¢ä¸ºå¼ é‡
    states = torch.tensor(np.array(states), dtype=torch.complex128)
    actions = torch.tensor(actions, dtype=torch.complex128).unsqueeze(-1)
    rewards = torch.tensor(rewards, dtype=torch.complex128)
    next_states = torch.tensor(np.array(next_states), dtype=torch.complex128)
    dones = torch.tensor(dones, dtype=torch.complex128)

    # è·å–å½“å‰åŠ¨ä½œçš„ Q å€¼
    current_q_values = model(states).gather(1, actions).squeeze()

    # ä½¿ç”¨ç›®æ ‡ç½‘ç»œé¢„æµ‹ä¸‹ä¸€çŠ¶æ€çš„æœ€å¤§ Q å€¼
    with torch.no_grad():
        next_q_values = target_model(next_states).max(1).values
        expected_q_values = rewards + (1 - dones) * gamma * next_q_values

    # è®¡ç®—æŸå¤±
    loss = nn.MSELoss()(current_q_values, expected_q_values)

    # åå‘ä¼ æ’­
    optimizer.zero_grad()
    loss.backward()

if __name__ == '__main__':
    # åˆå§‹çŠ¶æ€ï¼šå…¨éƒ¨åŸºæ€
    initial_state = [
        [1, 0],
        [1, 0],
        [1, 0],
        [1, 0]
    ]

    # åˆå§‹åŒ–æ¨¡å‹å’Œç›®æ ‡ç½‘ç»œ
    model = QNetwork(state_size, action_size)
    target_model = QNetwork(state_size, action_size)
    target_model.load_state_dict(model.state_dict())
    target_model.eval()

    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    buffer = ReplayBuffer(memory_size)

    epsilon = epsilon_start

    best_fidelity = float('inf')
    best_episode = 0
    best_actions = []

    print("å¼€å§‹è®­ç»ƒ...")

    for episode in range(episodes):
        state = initial_state.copy()
        total_reward = 0
        done = False
        action_history = []  # è®°å½•æœ¬ episode æ‰€æœ‰åŠ¨ä½œ

        for step_num in range(max_steps):
            flat_state = flatten_state(state)
            state_tensor = torch.tensor(flat_state, dtype=torch.complex128).unsqueeze(0)

            action_idx = choose_action(model, state_tensor, epsilon)
            action_history.append(action_idx)

            next_state, reward = step_1(state, action_idx)
            done = False

            flat_next_state = flatten_state(next_state)

            buffer.add(flat_state, action_idx, reward, flat_next_state, done)

            state = next_state
            total_reward += reward

            train(model, target_model, optimizer, buffer, batch_size, gamma)

            if done:
                break

        epsilon = max(epsilon_end, epsilon * epsilon_decay)

        if episode % 10 == 0:
            target_model.load_state_dict(model.state_dict())

        # æ¯æ¬¡ç»“æŸåï¼Œè®¡ç®—å½“å‰ fidelity
        final_vector = state_to_tensor_product(state)
        fidelity = compute_fidelity_reward(final_vector, target_state)

        if fidelity < best_fidelity:
            best_fidelity = fidelity
            best_episode = episode
            best_actions = action_history.copy()

        if 1 - fidelity < 0.0001:
            print("finished")
            break

        print(f"Episode {episode + 1}, Fidelity: {fidelity:.6f}, Epsilon: {epsilon:.2f}")

    # è¾“å‡ºæœ€ä½³ç»“æœ
    print("\n\nğŸ‰ æœ€ä½³ç»“æœå‡ºç°åœ¨ Episode", best_episode + 1)
    print("Fidelity:", best_fidelity)
    print("ä½¿ç”¨çš„åŠ¨ä½œåºåˆ—ï¼ˆç¼–å·ï¼‰:")
    print(best_actions)
    print("å¯¹åº”çš„åŠ¨ä½œä¸º:")
    for idx in best_actions:
        print(f"{idx}: {actions[idx]}")
